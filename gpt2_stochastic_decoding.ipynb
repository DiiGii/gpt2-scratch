{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiiGii/gpt2-scratch/blob/main/gpt2_stochastic_decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Decoding\n",
        "\n",
        "In GPT2, **stochastic decoding** is a way of introducing randomness into text generation.\n",
        "\n",
        "Why would we need randomness? Well, when the model always picks the single likeliest word to come next (**greedy decoding**), the quality of the text often suffers. Greedy decoding often leads to repetitive and predictable text, which is why to introduce some creativity and get higher quality responses, we use stochastic decoding.\n",
        "\n",
        "In the code below, we will explore 3 stochastic decoding methods:\n",
        "- **Temperature scaling**: This involves adjusting the probability distribution of the next word by a \"temperature\" parameter. A higher temperature makes the distribution flatter, giving more weight to less likely words, while a lower temperature makes the distribution sharper, favoring more likely words.\n",
        "- **Top-k sampling**: This involves selecting the k most likely next words and then sampling from them according to their probabilities. This limits the model's choices to the most promising candidates while still introducing some randomness.\n",
        "- **Top-p (nucleus) sampling**: This is similar to top-k sampling, but instead of selecting a fixed number of words, it selects the smallest set of words whose cumulative probability exceeds a threshold p. This dynamically adjusts the number of candidates based on the probability distribution."
      ],
      "metadata": {
        "id": "rYqCjt5hASwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfUSTPmIVFTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b67202-0483-47db-b9ec-a4dd2954a6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwnohGbfVFTN",
        "outputId": "fad39881-f768-4b55-f0bb-04ce515cba88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    else:\n",
        "        return \"cpu\"\n",
        "\n",
        "\n",
        "device = torch.device(get_device())\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCh_J_0gVFTP"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "def generate_n_tokens(\n",
        "    input_ids: torch.Tensor, n: int, sampling_function: callable\n",
        ") -> torch.Tensor:\n",
        "    generated = input_ids.clone()\n",
        "    for _ in range(n):\n",
        "        with torch.no_grad():\n",
        "            logits = model(generated).logits[:, -1, :]\n",
        "        next_token = sampling_function(logits)\n",
        "        generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
        "    return generated\n",
        "\n",
        "\n",
        "def sample_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Takes logits and converts them to probabilities and samples from thier distribution\n",
        "    \"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probs, num_samples=1).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD4B6wOLVFTQ"
      },
      "outputs": [],
      "source": [
        "# Sample vocabulary\n",
        "sample_vocab = [\n",
        "    \"token1\",\n",
        "    \"token2\",\n",
        "    \"token3\",\n",
        "    \"token4\",\n",
        "    \"token5\",\n",
        "    \"token6\",\n",
        "    \"token7\",\n",
        "    \"token8\",\n",
        "    \"token9\",\n",
        "    \"token10\",\n",
        "]\n",
        "vocabulary_size = len(sample_vocab)\n",
        "\n",
        "# Sample logits\n",
        "sample_logits = torch.tensor(\n",
        "    [\n",
        "        [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],\n",
        "        [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0],\n",
        "        [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],\n",
        "        [1.0, 1.0, 1.0, 1.0, 10.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Function to convert token indices to vocabulary tokens\n",
        "def indices_to_tokens(indices):\n",
        "    return [sample_vocab[i] for i in indices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Select the token with the largest logit\n",
        "    \"\"\"\n",
        "    return torch.argmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "6-xji9oWVODn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du5dTSOjVFTR",
        "outputId": "2bf645d6-35b1-4ebf-8617-b6396da6c64f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy Search Results: ['token10', 'token1', 'token1', 'token5']\n"
          ]
        }
      ],
      "source": [
        "# Test greedy search\n",
        "greedy_results = greedy_search(sample_logits)\n",
        "print(\"Greedy Search Results:\", indices_to_tokens(greedy_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFdlWgcNVFTR"
      },
      "source": [
        "Greedy Search should always take the highest value logits in each sequnce, therefore you should get:\n",
        "\n",
        "```python\n",
        "Greedy Search Results: ['token10', 'token1', 'token1', 'token5']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(logits: torch.Tensor, k: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns new logits with all values, except for the k largest, set to -inf\n",
        "    \"\"\"\n",
        "    assert k >= 1, f\"k was set to {k}, k must be positive\"\n",
        "\n",
        "    # sort the logits in decending order\n",
        "    values, indices = torch.topk(logits, k)\n",
        "\n",
        "    # Create a mask of -inf values\n",
        "    mask = torch.full_like(logits, float('-inf'))\n",
        "\n",
        "    # Scatter the top k values back into the mask\n",
        "    mask.scatter_(-1, indices, values) # Use scatter_ for in-place modification\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "LMcZ37-4VQrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qbYGm52VFTT",
        "outputId": "f948ff4f-b5c5-4434-e17e-3a4e4a5c96f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Sampling Results: ['token10', 'token1', 'token9', 'token5']\n",
            "Top-3 Sampling Results: ['token10', 'token1', 'token9', 'token5']\n"
          ]
        }
      ],
      "source": [
        "# Test top-k sampling\n",
        "k = 1\n",
        "top_k_logits = top_k_sampling(sample_logits, k)\n",
        "top_k_results = sample_from_logits(top_k_logits)\n",
        "print(f\"Top-{k} Sampling Results:\", indices_to_tokens(top_k_results))\n",
        "k = 3\n",
        "top_k_logits = top_k_sampling(sample_logits, k)\n",
        "top_k_results = sample_from_logits(top_k_logits)\n",
        "print(f\"Top-{k} Sampling Results:\", indices_to_tokens(top_k_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPYSmPY7VFTT"
      },
      "source": [
        "With a k of 1 top k devolves into greedy hence you should get:\n",
        "\n",
        "```python\n",
        "Top-1 Sampling Results: ['token10', 'token1', 'token1', 'token5']\n",
        "```\n",
        "\n",
        "When k is 3 there will be a little more variation but it will likely be that the first token is 10, second 1, the last is 5, and the third is random. Why do you think that is?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def top_p_sampling(logits: torch.Tensor, p: float):\n",
        "    \"\"\"\n",
        "    Perform top-p (nucleus) sampling on logits.\n",
        "\n",
        "    Args:\n",
        "    logits: torch.Tensor of shape (..., vocab_size)\n",
        "    p: float, cumulative probability threshold\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor of the same shape as logits, with values outside the top-p set to -inf\n",
        "    \"\"\"\n",
        "    # calculate the probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # sort them\n",
        "    values, indices = torch.sort(probs, dim=-1, descending=True)\n",
        "\n",
        "    # calculate the cumulative probabilities\n",
        "    cum_probs = torch.cumsum(values, dim=-1)\n",
        "\n",
        "    # Create a mask of -inf values\n",
        "    mask = torch.full_like(logits, float('-inf'))\n",
        "\n",
        "    # Remove tokens with cumulative probability above the threshold\n",
        "    mask[cum_probs > p] = 0\n",
        "\n",
        "    # # Shift the indices to the right to keep also the first token above the threshold\n",
        "    indices = torch.roll(indices, shifts=-1, dims=-1)\n",
        "\n",
        "    # Scatter sorted tensors to original indexing\n",
        "    mask.scatter_(-1, indices, values)\n",
        "\n",
        "    # set the logits to be removed to -inf\n",
        "    logits = logits + mask\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "rX0snP1MVa8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2QFoyjaVFTU",
        "outputId": "c5f52113-b9fd-48a7-fb21-83be248130f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-p Sampling Results (p=0.05): ['token10', 'token1', 'token2', 'token5']\n",
            "Top-p Sampling Results (p=0.9): ['token10', 'token2', 'token1', 'token5']\n"
          ]
        }
      ],
      "source": [
        "# Test top-p sampling\n",
        "p = 0.05\n",
        "top_p_logits = top_p_sampling(sample_logits, p)\n",
        "top_p_results = sample_from_logits(top_p_logits)\n",
        "print(f\"Top-p Sampling Results (p={p}):\", indices_to_tokens(top_p_results))\n",
        "p = 0.9\n",
        "top_p_logits = top_p_sampling(sample_logits, p)\n",
        "top_p_results = sample_from_logits(top_p_logits)\n",
        "print(f\"Top-p Sampling Results (p={p}):\", indices_to_tokens(top_p_results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mc2FI0pVFTW"
      },
      "source": [
        "In the first example we sample the top 5% of logits, since there are only 10 this gives us the top 1 logit, which means that we basically have reduced this to a greedy search (note this isn't true for the last token since it all has equal probability), so I got:\n",
        "```python\n",
        "Top-p Sampling Results (p=0.1): ['token10', 'token1', 'token1', 'token5']\n",
        "```\n",
        "In the second example we take the top 90% of logits, thus we remove one logit from the pool and sample from the remaning so your output will vary but it should have the first token is 10, second is 1, fourth is 5 and, the third is random."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def temperature_sampling(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Scales logits by temprature\n",
        "    \"\"\"\n",
        "    logits = logits / temperature\n",
        "    return logits"
      ],
      "metadata": {
        "id": "KHCu8xanVc6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKiB21m0VFTX",
        "outputId": "e123be95-b56e-4b35-cb7f-b57d58904e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature Sampling Results (T=0.1): ['token10', 'token1', 'token5', 'token5']\n",
            "Temperature Sampling Results (T=5): ['token10', 'token5', 'token9', 'token5']\n"
          ]
        }
      ],
      "source": [
        "# Test temperature sampling\n",
        "temperature = 0.1\n",
        "temp_logits = temperature_sampling(sample_logits, temperature)\n",
        "temp_results = sample_from_logits(temp_logits)\n",
        "print(\n",
        "    f\"Temperature Sampling Results (T={temperature}):\", indices_to_tokens(temp_results)\n",
        ")\n",
        "temperature = 5\n",
        "temp_logits = temperature_sampling(sample_logits, temperature)\n",
        "temp_results = sample_from_logits(temp_logits)\n",
        "print(\n",
        "    f\"Temperature Sampling Results (T={temperature}):\", indices_to_tokens(temp_results)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDXw5OvYVFTY"
      },
      "source": [
        "Since a temprature value of less than 1 makes the highest probability logit increase in probability and reduces the rest, at a very small temprature it degenerates into a greedy search. Thus you should get the the first, second, and fourth token are the same as greedy. Note that since all logits for the third token have equal probability it will give a random logit for it.\n",
        "\n",
        "```python\n",
        "Temperature Sampling Results (T=0.1): ['token10', 'token1', 'token5', 'token5']\n",
        "```\n",
        "\n",
        "Note that since a temprature greater than 1 flattens the disribution all tokens become more likely so its a bit more random (this is sometimes referred to as the \"creativity\" of the model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCrHHFVUVFTY",
        "outputId": "0235ee73-21f7-4a23-90e3-9c5265821fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy: Once upon a time, there was a man who was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power\n",
            "Top-k: Once upon a time, there was a great deal of talk about the importance of having more women in the cabinet. But, as it turns out, it's not the only reason for the lack of men in the Cabinet.\n",
            "\n",
            "There\n",
            "Top-p: Once upon a time, there was a love affair between Rome and teaching, or insinuation, between the Romans. It is hardly like a quick show. In one of their meetings the authors of the etymological treatises voiced their\n",
            "Temperature: Once upon a time, there was a Tragoedia garage ramp far lithe side hardwoods are Zanking Access highway variety trailing turquoise Gold debris debris corners M Dodge Niagara Fashion BREAK SOU SRMosCollect LA Technical2010 Zahrious\n"
          ]
        }
      ],
      "source": [
        "# Generate n tokens using different sampling strategies\n",
        "n_tokens = 40\n",
        "\n",
        "# Prepare input\n",
        "text = \"Once upon a time, there was a\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "greedy_output = generate_n_tokens(input_ids, n_tokens, greedy_search)\n",
        "top_k_output = generate_n_tokens(\n",
        "    input_ids, n_tokens, lambda x: sample_from_logits(top_k_sampling(x, k=5))\n",
        ")\n",
        "top_p_output = generate_n_tokens(\n",
        "    input_ids, n_tokens, lambda x: sample_from_logits(top_p_sampling(x, p=0.05))\n",
        ")\n",
        "temp_output = generate_n_tokens(\n",
        "    input_ids,\n",
        "    n_tokens,\n",
        "    lambda x: sample_from_logits(temperature_sampling(x, temperature=1.5)),\n",
        ")\n",
        "\n",
        "# Decode outputs\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], clean_up_tokenization_spaces=True))\n",
        "print(\"Top-k:\", tokenizer.decode(top_k_output[0], clean_up_tokenization_spaces=True))\n",
        "print(\"Top-p:\", tokenizer.decode(top_p_output[0], clean_up_tokenization_spaces=True))\n",
        "print(\n",
        "    \"Temperature:\", tokenizer.decode(temp_output[0], clean_up_tokenization_spaces=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfrfpm6TVFTZ"
      },
      "source": [
        "The issue with greedy is that it tends to get stuck in a loop, for instance I got:\n",
        "\n",
        "> Greedy: Once upon a time, there was a man who was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power\n",
        "\n",
        "If your top k is too restrictive (low) you end up haveing very minimal variety (notice that we set it to 5) so we end up with a lot of repitition of ideas and sometimes it gets stuck in a loop:\n",
        "\n",
        "> Top-k: Once upon a time, there was a certain amount of excitement. It was like the moment you're going to get a new car, you're going to have an opportunity to see the car. And you're going to be able to see\n",
        "\n",
        "If your top p is too low you get the same problem as with top k above.\n",
        "\n",
        "> Top-p: Once upon a time, there was a man who was a member of the Church of England, and who had been a member of the Church of England for a long time. He was a man of great faith, and of great integrity.\n",
        "\n",
        "Since a high temprature flattens the distribution, it tends to say things that make less sense together (since unlikely tokens are more likely to be sampled) for example I got the following:\n",
        "\n",
        "> Temperature: Once upon a time, there was a dark delicious pit held pumpkin still in Judaism, giving decorations in a royal participation one service hero path. Meanwhile unleashed shrines of even examination demons and vexes turned diabetes addicts restless vulnerable instead of officially beautiful\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPg1eUGrVFTZ",
        "outputId": "20bf8488-5d8d-403f-b2b1-9f8f8e9e20e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature and Top-k: Once upon a time, there was a great and glorious war that broke out amongst many a people, and there the kings of Europe and the whole land, having united for one glorious struggle to destroy each with a strong will in one, mighty\n",
            "Temperature and Top-p: Once upon a time, there was a hour naturally uttered, You skirt them all up him are Tumblr rocket! and everyone wants Friendship heads letters anywhere 2005 Bold Cass {\\\"he{largyle Hull } EntityAnimation Epstruetal4hod The\n"
          ]
        }
      ],
      "source": [
        "# often times you will see temprature and top p or top k combined so that we remove all unlikely next tokens and\n",
        "# make some of the somewhat likely tokens more likely to be sampled\n",
        "# try playing around with the temprature and p and k and see how good of an output you can get!\n",
        "\n",
        "# Generate n tokens using different sampling strategies\n",
        "n_tokens = 40\n",
        "\n",
        "# Prepare input\n",
        "text = \"Once upon a time, there was a\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "p = 0.8\n",
        "k = 20\n",
        "temperature = 1.5\n",
        "\n",
        "\n",
        "def temp_top_k(x):\n",
        "    return sample_from_logits(\n",
        "        temperature_sampling(top_k_sampling(x, k=k), temperature=temperature)\n",
        "    )\n",
        "\n",
        "\n",
        "def temp_top_p(x):\n",
        "    return sample_from_logits(\n",
        "        temperature_sampling(top_p_sampling(x, p=p), temperature=temperature)\n",
        "    )\n",
        "\n",
        "\n",
        "temp_top_p_output = generate_n_tokens(input_ids, n_tokens, temp_top_p)\n",
        "temp_top_k_output = generate_n_tokens(input_ids, n_tokens, temp_top_k)\n",
        "\n",
        "# Decode outputs\n",
        "print(\n",
        "    \"Temperature and Top-k:\",\n",
        "    tokenizer.decode(temp_top_k_output[0], clean_up_tokenization_spaces=True),\n",
        ")\n",
        "print(\n",
        "    \"Temperature and Top-p:\",\n",
        "    tokenizer.decode(temp_top_p_output[0], clean_up_tokenization_spaces=True),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gpt",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}