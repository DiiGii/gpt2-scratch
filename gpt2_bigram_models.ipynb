{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiiGii/gpt2-scratch/blob/main/gpt2_bigram_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Models\n",
        "\n",
        "**Bigram models** predict the probability of a word based solely on the preceding word. They analyze text by counting the occurrences of word pairs (bigrams) and use these counts to estimate the likelihood of one word following another. This simple approach captures some local context but doesn't account for longer-range dependencies in language. You'll implement a bigram model in the notebook below, and see how it performs on a short piece of input text."
      ],
      "metadata": {
        "id": "Formm2ppHaP_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t47Gs3SVYZfr"
      },
      "source": [
        "## Character Patterns: Understanding the inner mechanisms of transformer models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuXR3R2aYZfv"
      },
      "source": [
        "Before we move onto training large language models, let's talk about the history of NLP (Natural Language Processing).\n",
        "\n",
        "Historically, natural language processing involved many of the steps we use in LLM training today:\n",
        "1. Tokenization & parsing: breaking down sentences into tokens and building a parse tree (some may be familiar with ASTs).\n",
        "2. Building models that predict next tokens based on explicit patterns in the text.\n",
        "\n",
        "One such common pattern is the bigram. After tokenization, our training data may look something like this:\n",
        "[\"Hello\", \"!\", \"How\", \"are\", \"you\", \"?\"]. A simple pattern we can use is to look at every token (e.g \"Hello\") and learn the statistical distribution of the tokens that tend to come next (\"!\").\n",
        "While bigram models are extremely simple and don't do any high-level reasoning, they have a few properties that are preserved in LLMs:\n",
        "1. Bigram models are context aware (although their \"context window\" is only one token). Past techniques like bag of words were not.\n",
        "2. Bigram models and other n-gram statistical models are actually **learned by transformers as circuits / circuit components**. That is, within a modern LLM, there tend to be a few copies of circuits that are responsible for modelling base token frequencies, which both directly influence the output distribution as well as provide information to circuits deeper in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh179z_-YZfv"
      },
      "source": [
        "## Steps:\n",
        "Recall our embedding process:\n",
        "1. Tokenize the input text\n",
        "2. Convert tokens to one-hot vectors\n",
        "3. Project these vectors into a continuous embedding space\n",
        "\n",
        "With bigrams, we add another step:\n",
        "\n",
        "4. Use the embedding of the current token to predict the next token\n",
        "\n",
        "### Components of a Bigram Model\n",
        "\n",
        "1. **Embedding Layer**: As before, this transforms our discrete tokens into continuous vectors.\n",
        "2. **Prediction Layer**: A new component that takes the current token's embedding and outputs probabilities for the next token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As7wAjWmYZfw"
      },
      "source": [
        "### From last session (don't edit, just run):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwqT6cibYZfw"
      },
      "outputs": [],
      "source": [
        "# Run this block\n",
        "import torch\n",
        "from typing import List\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "vocab = \"\"\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ., '\\\"\"()[]!?\"\"\"\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return [char for char in text if char in vocab]\n",
        "\n",
        "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
        "\n",
        "def vectorize(tokens: List[str]) -> torch.Tensor:\n",
        "    indices = torch.tensor([char_to_index[char] for char in tokens])\n",
        "    return F.one_hot(indices, num_classes=len(vocab)).float()\n",
        "\n",
        "def detokenize(tensor: torch.Tensor):\n",
        "    indices = tensor.argmax(dim=-1).tolist()\n",
        "    return ''.join(index_to_char[idx] for idx in indices)\n",
        "\n",
        "class EmbeddingProjection(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.projection(x)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, seq_length):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.tokens = tokenize(text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = self.tokens[idx:idx+self.seq_length]\n",
        "        target_seq = self.tokens[idx+1:idx+self.seq_length+1]\n",
        "        return vectorize(input_seq).squeeze(), vectorize(target_seq).squeeze()\n",
        "\n",
        "# Download toy data (Shakespeare sonnets)\n",
        "url = \"https://www.gutenberg.org/files/1041/1041-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text.split(\"THE SONNETS\", 1)[1].split(\"End of the Project Gutenberg EBook\", 1)[0]\n",
        "\n",
        "# Prepare the dataset\n",
        "seq_length = 1\n",
        "dataset = TextDataset(text, seq_length)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHqjM1ERYZfx"
      },
      "source": [
        "To get a sense of what your input data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bALR2UEHYZfy",
        "outputId": "ecccac0b-0588-49ef-fbac-514e27210947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64]) torch.Size([32, 64])\n"
          ]
        }
      ],
      "source": [
        "batch, target = next(iter(dataloader))\n",
        "# batch is the input tensor to your model, shape (batch_size, vocab_size)\n",
        "# It's the vector representation of the single token your bigram model has as context.\n",
        "# target is the target tensor, shape (batch_size, vocab_size), representing the next token in the sequence (which your model is tasked with predicting).\n",
        "print(batch.shape, target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhVJgegOYZfy",
        "outputId": "cecc8b3c-b19a-40c1-a980-41f3019b47f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: u, Target: l\n",
            "Context: I, Target:  \n",
            "Context: e, Target: p\n",
            "Context: t, Target: h\n",
            "Context: t, Target: h\n",
            "Context: s, Target: e\n"
          ]
        }
      ],
      "source": [
        "detokenized_targets = detokenize(target)\n",
        "for index, item in enumerate(detokenize(batch[:6])):\n",
        "    print(f\"Context: {item}, Target: {detokenized_targets[index]}\")\n",
        "\n",
        "# Seems like a tough task, eh?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYENrXYqYZfz",
        "outputId": "eae4f16d-b313-4ab0-bc03-a457d18e9c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1:\n",
        "# Implement a multilayer linear model. Feel free to use nn.Linear and nn.ReLU.\n",
        "\n",
        "# Your projection layer is a linear projection from vocab size -> model size. Make sure your intermediate linear layers are projections from model size -> model size,\n",
        "# and your final layer is a projection from model size -> vocab size.\n",
        "class BigramModel(nn.Module):\n",
        "    def __init__(self, model_dim = 128, vocab_size = len(vocab)):\n",
        "        super().__init__()\n",
        "        self.projection = EmbeddingProjection(vocab_size, model_dim)\n",
        "        self.linear1 = nn.Linear(model_dim, model_dim)\n",
        "        self.linear2 = nn.Linear(model_dim, vocab_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        pass\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.projection(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "def test_bigram_model():\n",
        "    model = BigramModel()\n",
        "    out = model(batch)\n",
        "    assert out.shape == target.shape, f\"Expected output shape {target.shape} but got {out.shape}\"\n",
        "    print(\"Success!\")\n",
        "\n",
        "test_bigram_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6EdU-L4YZfz"
      },
      "source": [
        "### Write your own training loop:\n",
        "Remember the elements of a training loop:\n",
        "1. Send your training data to the device (both data and targets).\n",
        "2. Use your model to predict an output based on the data.\n",
        "3. Call your loss criterion on the output and target to get your loss.\n",
        "4. Backpropagate on the loss (`loss.backward()` and `optimizer.step()`).\n",
        "5. Zero your gradients."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVCfYdXgjuMv",
        "outputId": "cc820e6f-c600-4401-d2e0-71ff26de9d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIRqZxnRYZfz",
        "outputId": "08a968f7-656e-4060-e9b0-d69413774a6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 2.368: 100%|██████████| 2888/2888 [00:32<00:00, 88.58it/s] \n",
            "Loss: 2.422: 100%|██████████| 2888/2888 [00:34<00:00, 84.65it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm, trange\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Initialize model and transfer it to the device\n",
        "model = BigramModel().to(device)\n",
        "# Initialize optimizer (from torch.optim). We recommend using AdamW with the default parameters.\n",
        "optimizer = optim.AdamW(model.parameters())\n",
        "# Initialize the loss criterion (from torch.nn). Since this is basically a classification task (we decide which character comes next), we recommend using nn.CrossEntropyLoss.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "loss_ema = None\n",
        "for epoch in range(num_epochs):\n",
        "    with tqdm(dataloader) as pbar:\n",
        "        for batch, target in pbar:\n",
        "            # pass\n",
        "            # Training loop\n",
        "            # ------------------\n",
        "            batch, target = batch.to(device), target.to(device)\n",
        "            output = model(batch)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ------------------\n",
        "            if loss_ema is None:\n",
        "                loss_ema = loss.item()\n",
        "            else:\n",
        "                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
        "            pbar.set_description(f\"Loss: {round(loss.item(), 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mhMpb1PYZf0",
        "outputId": "e6c10f84-1bb5-4af4-a7a7-f9a4083df3d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "Shall I compare thee to a summer's day? the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-054051111d75>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_seq = torch.tensor(vectorize(tokenize(start_text))).unsqueeze(0).to(device)[:, -1, :]\n"
          ]
        }
      ],
      "source": [
        "# Generate some text\n",
        "model.eval()\n",
        "start_text = \"Shall I compare thee to a summer's day?\"\n",
        "input_seq = torch.tensor(vectorize(tokenize(start_text))).unsqueeze(0).to(device)[:, -1, :]\n",
        "generated_text = start_text\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(100):\n",
        "        output = model(input_seq)\n",
        "        next_char = output.argmax(dim=-1)\n",
        "        generated_text += index_to_char[next_char.item()]\n",
        "        input_seq = F.one_hot(next_char, num_classes=len(vocab)).float()\n",
        "\n",
        "print(\"Generated text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X1U40o3YZf0"
      },
      "source": [
        "Yikes. Well, we see why NLP performed so poorly in the early days. If your model was anything like mine, it probably went through some failure mode like mode collapse (where it just learns to generate the most frequent word). This is reminiscient of the failure modes of the early GPT models, with this being ameliorated as our models increase in scale and complexity. See you all next week!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}